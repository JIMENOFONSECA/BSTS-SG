{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from config import INTERVENTION_CALENDAR, DATA_CONSUMPTION_RAW_FOLDER, DATA_METADATA_PROCESSED_FILE, DATA_HOLIDAYS_PROCESSED_FILE, DATA_CONSUMPTION_PROCESSED_FILE,DATA_METADATA_PROCESSED_FILE\n",
    "from config import DATA_CONSUMPTION_PROCESSED_FILE,DATA_CONSUMPTION_SEMI_PROCESSED_FILE,  DATA_WEATHER_PROCESSED_FILE,  DATA_METADATA_PROCESSED_FILE, DATA_HOLIDAYS_PROCESSED_FILE, DATA_ISO_CONSUMPTION_PROCESSED_FILE, DATA_ENTHALPY_GRADIENTS_PROCESSED_FILE, DATA_SOLAR_GAINS_PROCESSED_FILE, DATA_GBM_CONSUMPTION_PROCESSED_FILE\n",
    "from lgbm_imputer import imputer\n",
    "import matplotlib\n",
    "import warnings\n",
    "from auxiliary import week_of_month\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interventions = INTERVENTION_CALENDAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join to metadata\n",
    "metadata_df = pd.read_excel(DATA_METADATA_PROCESSED_FILE, sheets='SENSORS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_faulty = pd.read_excel(DATA_METADATA_PROCESSED_FILE, sheet_name=\"FAULTY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return time.asctime(time.localtime(x/1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headdir1 = os.path.join(DATA_CONSUMPTION_RAW_FOLDER, 'DAILY_ntufrs1')\n",
    "headdir2 = os.path.join(DATA_CONSUMPTION_RAW_FOLDER, 'DAILY_ntufrs2')\n",
    "headdir3 = os.path.join(DATA_CONSUMPTION_RAW_FOLDER, 'DAILY_ntufrs3')\n",
    "headdirs = [headdir1,headdir2, headdir3]\n",
    "data_holidays = DATA_HOLIDAYS_PROCESSED_FILE\n",
    "data_output_path = DATA_CONSUMPTION_PROCESSED_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the data that exists\n",
    "data_raw = pd.DataFrame()\n",
    "for headdir in headdirs:\n",
    "    list_data_files = os.listdir(headdir)\n",
    "    for file in list_data_files:\n",
    "        path_to_data = os.path.join(headdir,file)\n",
    "        with open(path_to_data) as json_file:\n",
    "            data = json.load(json_file)\n",
    "            smapees_in_data = list(data.keys())\n",
    "            for smapee in smapees_in_data:\n",
    "                try:\n",
    "                    data_clean = pd.DataFrame(data[smapee]['consumptions'])[['timestamp', 'consumption']]\n",
    "                    data_clean['smapee'] = \"ID\"+str(int(smapee))\n",
    "                    data_clean['CONSUMPTION_kWh'] = data_clean['consumption']/1000.0\n",
    "                    data_clean = data_clean.drop('consumption', axis=1)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                data_raw = data_raw.append(data_clean, ignore_index=True)\n",
    "data_raw['timestamp'] = pd.to_datetime(data_raw['timestamp'].apply(lambda x: f(x)))\n",
    "data_raw = data_raw.drop_duplicates(['smapee','timestamp'])\n",
    "data_raw.reset_index(inplace=True, drop=True)\n",
    "data_raw.smapee.count(), len(data_raw.smapee.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smapee = 'ID27968'\n",
    "# data = data_raw.set_index('timestamp')\n",
    "# c = data[data['smapee']==smapee]\n",
    "# length = c.shape[0]\n",
    "# miss = c[['CONSUMPTION_kWh']].isnull().sum()*100/length\n",
    "# ax = c[['CONSUMPTION_kWh']].plot()\n",
    "# ax.legend([str(smapee)+' '+str(miss)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data_raw.set_index('timestamp')\n",
    "# trend = data['smapee'].unique()\n",
    "# for smapee in trend:\n",
    "#     c = data[data['smapee']==smapee]\n",
    "#     length = c.shape[0]\n",
    "#     miss = c[['CONSUMPTION_kWh']].isnull().sum()*100/length\n",
    "#     ax = c[['CONSUMPTION_kWh']].plot()\n",
    "#     ax.legend([str(smapee)+' '+str(miss)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include all the dates that are missing (for now we do from the first date of intervention to the latest one)\n",
    "unique_smapees = data_raw['smapee'].unique()\n",
    "data_raw_full_extent = data_raw.set_index(['smapee', 'timestamp'], drop=False).sort_index()\n",
    "\n",
    "#range of all the thee experiments, min and max\n",
    "intervention_data = interventions[1]\n",
    "earliest_date = '2018-01-01 00:00'\n",
    "intervention_data = interventions[3]\n",
    "latest_date = '2019-12-31 23:00'\n",
    "range_experiments = pd.date_range(start=earliest_date, end=latest_date, freq='D')\n",
    "\n",
    "# construct full index w/o missing dates\n",
    "full_index = pd.MultiIndex.from_product([unique_smapees, range_experiments])\n",
    "data_raw_full_extent = data_raw_full_extent.reindex(full_index)\n",
    "data_raw_full_extent['smapee'] = data_raw_full_extent.index.get_level_values(0)\n",
    "data_raw_full_extent['timestamp'] = data_raw_full_extent.index.get_level_values(1)\n",
    "data_raw_full_extent.reset_index(inplace=True, drop=True)\n",
    "data_raw_full_extent.smapee.count(), len(data_raw_full_extent.smapee.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#erase faulty and fill in missing dates\n",
    "unique_smapees = data_raw_full_extent['smapee'].unique()\n",
    "data_raw_full_extent_after_faulty = data_raw_full_extent.copy()\n",
    "for smapee in unique_smapees:\n",
    "    faulty = meta_data_faulty[meta_data_faulty['smapee']==smapee]\n",
    "    if faulty.empty == False:\n",
    "        #print(\"filling range of data in sensor with null\", smapee)\n",
    "        range_na = pd.date_range(start=faulty['invalid from'].values[0], end=faulty['invalid to'].values[0], freq='D')\n",
    "        data_raw_full_extent_after_faulty.loc[(data_raw_full_extent_after_faulty['timestamp'].isin(range_na))&(data_raw_full_extent_after_faulty['smapee']==smapee), 'CONSUMPTION_kWh'] = np.nan\n",
    "        \n",
    "        #try the second range\n",
    "        if pd.isnull(faulty['2invalid from'].values[0]) == False:  \n",
    "            #print(\"filling range of data in sensor with null\", smapee)\n",
    "            range_na = pd.date_range(start=faulty['2invalid from'].values[0], end=faulty['2invalid to'].values[0], freq='D')\n",
    "            data_raw_full_extent_after_faulty.loc[(data_raw_full_extent_after_faulty['timestamp'].isin(range_na))&(data_raw_full_extent_after_faulty['smapee']==smapee), 'CONSUMPTION_kWh'] = np.nan\n",
    "    \n",
    "    #if explicityl we say to discard these values\n",
    "    if faulty['DSICARD'].values == True:\n",
    "        print(\"discarding sensor because it is corrupted\", smapee)\n",
    "        data_raw_full_extent_after_faulty = data_raw_full_extent_after_faulty[data_raw_full_extent_after_faulty['smapee'] != smapee]\n",
    "data_raw_full_extent_after_faulty.reset_index(inplace=True, drop=True)\n",
    "len(data_raw_full_extent_after_faulty.smapee.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = data_raw_full_extent_after_faulty.set_index('timestamp')\n",
    "c = c[c['smapee']==\"ID28099\"]\n",
    "length = c.shape[0]\n",
    "miss = c[['CONSUMPTION_kWh']].isnull().sum()*100/length\n",
    "ax = c[['CONSUMPTION_kWh']].plot()\n",
    "ax.legend([str(28099)+' '+str(miss)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of smapees despues de eliminar faulty\n",
    "unique_smapees = data_raw_full_extent_after_faulty['smapee'].unique()\n",
    "len(unique_smapees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#erase outliers (99th percentile)\n",
    "quantile = data_raw_full_extent_after_faulty.groupby('smapee').quantile(0.99)\n",
    "for smapee in unique_smapees:\n",
    "    thpercentile = quantile.loc[smapee,'CONSUMPTION_kWh']\n",
    "    consumption = data_raw_full_extent_after_faulty.loc[data_raw_full_extent_after_faulty['smapee']==smapee,'CONSUMPTION_kWh'].values\n",
    "    data_raw_full_extent_after_faulty.loc[data_raw_full_extent_after_faulty['smapee']==smapee,'CONSUMPTION_kWh'] = [np.nan if x >= thpercentile else x for x in consumption] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_raw_full_extent_after_faulty.set_index('timestamp')\n",
    "for smapee in unique_smapees:\n",
    "    c = data[data['smapee']==smapee]\n",
    "    length = c.shape[0]\n",
    "    miss = c[['CONSUMPTION_kWh']].isnull().sum()*100/length\n",
    "    ax = c[['CONSUMPTION_kWh']].plot()\n",
    "    ax.legend([str(smapee)+' '+str(miss)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now get flags to know what sensors can be used for the experiments\n",
    "interventions = INTERVENTION_CALENDAR\n",
    "clean_data = data_raw_full_extent_after_faulty.set_index('timestamp')\n",
    "\n",
    "#create empty \n",
    "metadata_df['VALID_FOR_BOTH'] = False\n",
    "metadata_df['VALID_FOR_PRE'] = False\n",
    "metadata_df['VALID_FOR_POST'] = False\n",
    "\n",
    "#threshold to reject\n",
    "threshold_pre = 0.65\n",
    "threshold_postperiod = 0.85\n",
    "\n",
    "#iterate over metadata:\n",
    "lenght = metadata_df.shape[0]\n",
    "for record in range(lenght):\n",
    "    #get what record to analyze\n",
    "    smapee = metadata_df.loc[record,'smapee']\n",
    "    experiment = metadata_df.loc[record,'EXPERIMENT']\n",
    "    smapee_tested = clean_data[clean_data['smapee']==smapee]\n",
    "    \n",
    "    #check if it complies, i.e., is there sufficient data for the post-period\n",
    "    intervention_data = interventions[experiment]\n",
    "    \n",
    "    pre_period = intervention_data[1]\n",
    "    range_pre_intervention_period = pd.date_range(start=pre_period[0], end=pre_period[1], freq='D')\n",
    "    records_all_pre_period = len(range_pre_intervention_period)\n",
    "    records_it_has_pre = len(smapee_tested[(smapee_tested.index.isin(range_pre_intervention_period))&(smapee_tested['CONSUMPTION_kWh']>0.0)])\n",
    "    perc_it_has_pre = records_it_has_pre /records_all_pre_period\n",
    "    \n",
    "    post_period = intervention_data[2]\n",
    "    range_post_intervention_period = pd.date_range(start=post_period[0], end=post_period[1], freq='D')\n",
    "    records_all_post_period = len(range_post_intervention_period)\n",
    "    records_it_has_post = len(smapee_tested[(smapee_tested.index.isin(range_post_intervention_period))&(smapee_tested['CONSUMPTION_kWh']>0.0)])\n",
    "    perc_it_has_post = records_it_has_post /records_all_post_period\n",
    "    \n",
    "    \n",
    "    if perc_it_has_post > threshold_postperiod:\n",
    "        metadata_df.loc[record,'VALID_FOR_POST'] = True\n",
    "        #print(\"this smpaee {} has {} % of the data in postperiod of in experiment {} and {} % data in preperiod\".format(smapee, perc_it_has_post*100, experiment, perc_it_has_pre*100))\n",
    "    \n",
    "    if perc_it_has_pre > threshold_pre:\n",
    "        metadata_df.loc[record,'VALID_FOR_PRE'] = True\n",
    "        #print(\"this smpaee {} has {} % of the data in postperiod of in experiment {} and {} % data in preperiod\".format(smapee, perc_it_has_post*100, experiment, perc_it_has_pre*100))\n",
    "    \n",
    "    if metadata_df.loc[record,'VALID_FOR_PRE'] & metadata_df.loc[record,'VALID_FOR_POST']:\n",
    "        metadata_df.loc[record,'VALID_FOR_BOTH'] = True\n",
    "    \n",
    "#print Valid:\n",
    "def get_number_valid_per_experiment(metadata_df, experiment, column_name):\n",
    "    exp1 = metadata_df[metadata_df['EXPERIMENT'] == experiment]\n",
    "    num_tot = exp1.shape[0]\n",
    "    num_valid = (exp1[column_name]==True).sum()\n",
    "    perc = num_valid*100/num_tot\n",
    "    print(\"the number of valid for exp. {} is {} out of {} or {}%\".format(experiment, num_valid, num_tot,perc))\n",
    "\n",
    "print(\"HAS DATA FOR THE PRE-INTERVENTION PERIOD\")\n",
    "get_number_valid_per_experiment(metadata_df, 1, 'VALID_FOR_PRE')\n",
    "get_number_valid_per_experiment(metadata_df, 2, 'VALID_FOR_PRE')\n",
    "get_number_valid_per_experiment(metadata_df, 3, 'VALID_FOR_PRE')\n",
    "\n",
    "print(\"HAS DATA FOR THE INTERVENTION AND POSTINTERVENTION PERIOD\")\n",
    "get_number_valid_per_experiment(metadata_df, 1, 'VALID_FOR_POST')\n",
    "get_number_valid_per_experiment(metadata_df, 2, 'VALID_FOR_POST')\n",
    "get_number_valid_per_experiment(metadata_df, 3, 'VALID_FOR_POST')\n",
    "\n",
    "print(\"HAS DATA FOR THE BOTH PERIODS PERIOD\")\n",
    "get_number_valid_per_experiment(metadata_df, 1, 'VALID_FOR_BOTH')\n",
    "get_number_valid_per_experiment(metadata_df, 2, 'VALID_FOR_BOTH')\n",
    "get_number_valid_per_experiment(metadata_df, 3, 'VALID_FOR_BOTH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw_full_extent_after_faulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LETS START WITH THE IMPUTATION WHERE VALUES ARE VALID FOR BOTH\n",
    "holidays_df = pd.read_csv(DATA_HOLIDAYS_PROCESSED_FILE)\n",
    "holidays_df['timestamp'] = pd.to_datetime(holidays_df['timestamp'])\n",
    "gradients_df = pd.read_csv(DATA_ENTHALPY_GRADIENTS_PROCESSED_FILE)\n",
    "gradients_df['timestamp'] = pd.to_datetime(gradients_df['timestamp'])\n",
    "\n",
    "#make merge\n",
    "data_very_clean = data_raw_full_extent_after_faulty.merge(metadata_df, left_on='smapee', right_on='smapee')\n",
    "# data_very_clean = data_very_clean.merge(holidays_df, left_on='timestamp', right_on='timestamp', how='left')\n",
    "# data_very_clean = data_very_clean.merge(gradients_df, left_on='timestamp', right_on='timestamp', how='left')\n",
    "data_very_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid\n",
    "data_very_clean[data_very_clean['VALID_FOR_BOTH']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "def model_imputer(data_train):\n",
    "    # lgb needs values that start in 0 if they are categorical\n",
    "    data_train['year'] = np.array(data_train['timestamp'].dt.year, dtype=np.uint16)\n",
    "    data_train['month'] = np.array(data_train['timestamp'].dt.month, dtype=np.uint8) - 1\n",
    "    data_train['dayofweek'] = np.array(data_train['timestamp'].dt.dayofweek, dtype=np.uint8)\n",
    "    data_train['dayofyear'] = np.array(data_train['timestamp'].dt.dayofyear, dtype=np.uint16) - 1\n",
    "    data_train['weekofyear'] = np.array(data_train['timestamp'].dt.weekofyear, dtype=np.uint8) - 1\n",
    "    data_train['weekday'] = data_train['dayofweek'].apply(lambda x: 1 if 0<=x<5 else 0)\n",
    "    data_train['calendar_wom'] = data_train['timestamp'].apply(week_of_month)\n",
    "    \n",
    "    target_feature_name = 'CONSUMPTION_kWh'\n",
    "    numerical_features_list = ['DEG_C_kJperKg',\n",
    "                               'DEG_DEHUM_kJperKg']\n",
    "    categorical_features_list = ['smapee',\n",
    "                                 'dayofweek',\n",
    "                                 'weekday',\n",
    "                                 'dayofyear',\n",
    "                                 'calendar_wom',\n",
    "                                 'teaching_time',\n",
    "                                 'school_holiday',\n",
    "                                 'holiday',\n",
    "                                 'month']\n",
    "    \n",
    "    for c in categorical_features_list:\n",
    "        data_train[c] = data_train[c].astype('category')\n",
    "        \n",
    "    id_column = 'smapee'\n",
    "    get_best_parameters = False\n",
    "    window = 7\n",
    "    params = {'learning_rate': 0.1,\n",
    "              'num_leaves': 31,\n",
    "              'max_depth':-1,\n",
    "              'min_data_in_leaf': 20,\n",
    "              'num_iterations': 10000,\n",
    "              'objective': 'rmse',\n",
    "              'metric': 'rmse'}\n",
    "    restored = imputer(df=data_train,\n",
    "                       timestamp_feature_name='timestamp',\n",
    "                       target_feature_name = target_feature_name,\n",
    "                       numerical_features_list = numerical_features_list,\n",
    "                       categorical_features_list = categorical_features_list,\n",
    "                       id_column = id_column,\n",
    "                       window = window,\n",
    "                       get_best_parameters = get_best_parameters,\n",
    "                       params=params)\n",
    "    return restored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment = 1\n",
    "# result_imputer = []\n",
    "# for experiment in [1,2,3]:\n",
    "#     #create training data to also impute\n",
    "#     intervention_data = interventions[experiment]\n",
    "#     pre_period = intervention_data[1]\n",
    "#     post_period = intervention_data[2]\n",
    "#     period = pd.date_range(start=pre_period[0], end=post_period[1], freq='D')\n",
    "    \n",
    "#     # get it from veryclean when all is valid\n",
    "#     data_train = data_very_clean[data_very_clean['VALID_FOR_BOTH']==True]\n",
    "#     data_train = data_train[data_train['timestamp'].isin(period)]\n",
    "#     data_train = data_train[data_train['EXPERIMENT']==experiment]\n",
    "#     data_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#     #train\n",
    "#     result_imputer.append(model_imputer(data_train))\n",
    "\n",
    "data_train = data_very_clean[data_very_clean['VALID_FOR_BOTH']==True]\n",
    "result_imputer = model_imputer(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save this for further use in the monthly values\n",
    "output_path = DATA_CONSUMPTION_SEMI_PROCESSED_FILE\n",
    "result_imputer[['timestamp', 'CONSUMPTION_kWh', 'smapee', 'INTERVENTION', 'ID_CEA']].to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of smapees\n",
    "for experiment in [1,2,3]:\n",
    "    #create training data to also impute\n",
    "    intervention_data = interventions[experiment]\n",
    "    pre_period = intervention_data[1]\n",
    "    post_period = intervention_data[2]\n",
    "    period = pd.date_range(start=pre_period[0], end=post_period[1], freq='D')\n",
    "    \n",
    "    if experiment == 1:\n",
    "        # get it from veryclean when all is valid\n",
    "        data_experiment1 = result_imputer[result_imputer['timestamp'].isin(period)]\n",
    "        data_experiment1 = data_experiment1[data_experiment1['EXPERIMENT']==experiment]\n",
    "        data_experiment1.reset_index(inplace=True, drop=True)\n",
    "    elif experiment == 2:\n",
    "        # get it from veryclean when all is valid\n",
    "        data_experiment2 = result_imputer[result_imputer['timestamp'].isin(period)]\n",
    "        data_experiment2 = data_experiment2[data_experiment2['EXPERIMENT']==experiment]\n",
    "        data_experiment2.reset_index(inplace=True, drop=True)\n",
    "    else:\n",
    "        # get it from veryclean when all is valid\n",
    "        data_experiment3 = result_imputer[result_imputer['timestamp'].isin(period)]\n",
    "        data_experiment3 = data_experiment3[data_experiment3['EXPERIMENT']==experiment]\n",
    "        data_experiment3.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual = data_experiment1.set_index('timestamp')\n",
    "visual = visual[visual['smapee']==\"ID28088\"]\n",
    "visual = visual[['CONSUMPTION_kWh', 'CONSUMPTION_kWh_all_imputed', 'CONSUMPTION_kWh_imputed']]\n",
    "font = {'family' : 'Arial',\n",
    "        'size'   : 18}\n",
    "ax = visual.plot()\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"Daily Consumption [kWh]\")\n",
    "ax.legend([\"Measured\", \"Imputed\"]);\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual = data_experiment3.set_index('timestamp')\n",
    "visual = visual[visual['smapee']==\"ID27785\"]\n",
    "visual = visual[['CONSUMPTION_kWh', 'CONSUMPTION_kWh_all_imputed', 'CONSUMPTION_kWh_imputed']]\n",
    "font = {'family' : 'Arial',\n",
    "        'size'   : 18}\n",
    "ax = visual.plot()\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"Daily Consumption [kWh]\")\n",
    "ax.legend([\"Measured\", \"Imputed\"]);\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_experiment1.loc[data_experiment1['CONSUMPTION_kWh'] < 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_experiment2.loc[data_experiment2['CONSUMPTION_kWh'] < 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_experiment3.loc[data_experiment3['CONSUMPTION_kWh'] < 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_experiment1.set_index('timestamp')\n",
    "smapees = data['smapee'].unique()\n",
    "for smapee in smapees:\n",
    "    c = data[data['smapee']==smapee]\n",
    "    length = c.shape[0]\n",
    "    miss = c[['CONSUMPTION_kWh']].isnull().sum()*100/length\n",
    "    ax = c[['CONSUMPTION_kWh','CONSUMPTION_kWh_imputed']].plot()\n",
    "    ax.legend([str(smapee)+' '+str(miss)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_experiment2.set_index('timestamp')\n",
    "smapees = data['smapee'].unique()\n",
    "for smapee in smapees:\n",
    "    c = data[data['smapee']==smapee]\n",
    "    length = c.shape[0]\n",
    "    miss = c[['CONSUMPTION_kWh']].isnull().sum()*100/length\n",
    "    ax = c[['CONSUMPTION_kWh','CONSUMPTION_kWh_imputed']].plot()\n",
    "    ax.legend([str(smapee)+' '+str(miss)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_experiment3.set_index('timestamp')\n",
    "smapees = data['smapee'].unique()\n",
    "for smapee in smapees:\n",
    "    c = data[data['smapee']==smapee]\n",
    "    length = c.shape[0]\n",
    "    miss = c[['CONSUMPTION_kWh']].isnull().sum()*100/length\n",
    "    ax = c[['CONSUMPTION_kWh','CONSUMPTION_kWh_imputed']].plot()\n",
    "    ax.legend([str(smapee)+' '+str(miss)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GATHER ALL RESULTS INTO ONE DATAFRAME\n",
    "restored = pd.DataFrame()\n",
    "result_imputer2 = [data_experiment1, data_experiment2, data_experiment3]\n",
    "for data_experiment in result_imputer2:\n",
    "    restored = restored.append(data_experiment, ignore_index=True)\n",
    "\n",
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2)) \n",
    "\n",
    "# #CEHCK THAT VALUES IN EXPERIMENT 1 and 3 are all the same smappees\n",
    "final = data_experiment2 #(which is clean)\n",
    "for treatment in ['T1', 'T2', 'T3', 'CONTROL']:\n",
    "    print(treatment)\n",
    "    lst1 = list(restored[(restored['EXPERIMENT']==1) & (restored['TREATMENT']== treatment)]['smapee'].unique())\n",
    "    lst2 = list(restored[(restored['EXPERIMENT']==3) & (restored['TREATMENT']== treatment)]['smapee'].unique())\n",
    "    intersect =  intersection(lst1, lst2)\n",
    "    print(len(intersect))\n",
    "    fin1 = restored[(restored['EXPERIMENT'] == 1) & (restored['TREATMENT']== treatment) & (restored['smapee'].isin(intersect))]\n",
    "    fin2 = restored[(restored['EXPERIMENT'] == 3) & (restored['TREATMENT']== treatment) & (restored['smapee'].isin(intersect))]\n",
    "    \n",
    "    final = final.append(fin1, ignore_index=True)\n",
    "    final = final.append(fin2, ignore_index=True)\n",
    "    \n",
    "final[['timestamp', 'CONSUMPTION_kWh', 'smapee', 'INTERVENTION']].to_csv(data_output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final[final[\"TREATMENT\"]==\"CONTROL\"].smapee.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET COUNT OF IMPUTED\n",
    "for data_experiment in result_imputer2:\n",
    "    valid = len(data_experiment['smapee'].unique())\n",
    "    no_imputed = data_experiment['CONSUMPTION_kWh_imputed'].isnull().sum()\n",
    "    total = data_experiment.shape[0]\n",
    "    perc_imputed = (total - no_imputed)*100/total\n",
    "    print(perc_imputed, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(data_experiment1[(data_experiment1['EXPERIMENT']==1) & (restored['TREATMENT']== 'T1')]['smapee'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
